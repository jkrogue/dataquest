{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(pixels):\n",
    "    sqrt = int(np.sqrt(pixels.shape[0]))\n",
    "    return pixels.copy().reshape(sqrt,sqrt)\n",
    "\n",
    "def visualize(data, index):\n",
    "    pixels = data.data[index]\n",
    "    label = data.target[index]\n",
    "    pixels = reshape(pixels)\n",
    "    plt.title(label)\n",
    "    plt.imshow(pixels, cmap='gray_r')\n",
    "    \n",
    "def visualize(data, st_idx, end_idx):\n",
    "    n_images = end_idx - st_idx\n",
    "    for i in range(st_idx, end_idx):\n",
    "        subplot_idx = i - st_idx + 1\n",
    "        ax = plt.subplot(n_images,1,subplot_idx)\n",
    "        pixels = data.data[i]\n",
    "        label = data.target[i]\n",
    "        pixels = reshape(pixels)\n",
    "        ax.imshow(pixels, cmap='gray_r')\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(digits, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)#for consistent results\n",
    "shuffled_index = np.random.permutation(range(X.shape[0]))\n",
    "shuffled_X = X[shuffled_index]\n",
    "shuffled_y = y[shuffled_index]\n",
    "train_test_split = int(X.shape[0] * 0.85)\n",
    "\n",
    "train_X = shuffled_X[:train_test_split]\n",
    "train_y = shuffled_y[:train_test_split]\n",
    "test_X = shuffled_X[train_test_split:]\n",
    "test_y = shuffled_y[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(clf):\n",
    "    global train_X, train_y, test_X, test_y\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "    train_preds = clf.predict(train_X)\n",
    "    test_preds = clf.predict(test_X)\n",
    "    acc = {}\n",
    "    acc['train'] = accuracy_score(train_y, train_preds)\n",
    "    acc['test'] = accuracy_score(test_y, test_preds)\n",
    "    return acc\n",
    "\n",
    "def print_acc(model_name, acc):\n",
    "    print (\"\"\"\n",
    "{}\n",
    "Train: {:.2%}\\tTest: {:.2%}\"\"\".format(model_name,acc['train'],acc['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Train: 99.74%\tTest: 94.44%\n"
     ]
    }
   ],
   "source": [
    "log_acc = model(LogisticRegression())\n",
    "print_acc(\"Logistic Regression\",log_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression alone works pretty well here, with accuracies ~94.4%!  There is some overfitting, let's see if we can improve with increasing weight of L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Train: 98.30%\tTest: 97.04%\n"
     ]
    }
   ],
   "source": [
    "log_acc = model(LogisticRegression(C=0.01))\n",
    "print_acc(\"Logistic Regression\",log_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through regularization we were able to improve results to 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single unit nn (logistic regression)\n",
      "Train: 99.87%\tTest: 94.81%\n"
     ]
    }
   ],
   "source": [
    "nn_log = model(MLPClassifier(hidden_layer_sizes=(), max_iter=1000,random_state=1))\n",
    "print_acc('Single unit nn (logistic regression)',nn_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, single output neuron network has performance similar to logistic regression as that's all it's doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP with single hidden layer\n",
      "\n",
      "1 hidden units\n",
      "Train: 28.81%\tTest: 24.07%\n",
      "\n",
      "8 hidden units\n",
      "Train: 98.10%\tTest: 91.48%\n",
      "\n",
      "16 hidden units\n",
      "Train: 100.00%\tTest: 96.30%\n",
      "\n",
      "32 hidden units\n",
      "Train: 100.00%\tTest: 96.30%\n",
      "\n",
      "64 hidden units\n",
      "Train: 100.00%\tTest: 97.78%\n",
      "\n",
      "128 hidden units\n",
      "Train: 100.00%\tTest: 96.67%\n",
      "\n",
      "256 hidden units\n",
      "Train: 100.00%\tTest: 97.04%\n",
      "\n",
      "512 hidden units\n",
      "Train: 100.00%\tTest: 97.41%\n",
      "\n",
      "1024 hidden units\n",
      "Train: 100.00%\tTest: 97.04%\n"
     ]
    }
   ],
   "source": [
    "neurons = [1,8,16,32,64,128,256,512,1024]\n",
    "accuracies = []\n",
    "aucs = []\n",
    "print(\"MLP with single hidden layer\")\n",
    "for each in neurons:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(each), max_iter=1000,activation='relu', random_state=1)\n",
    "    acc = model(nn)\n",
    "    print_acc('{} hidden units'.format(each), acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set accuracy increases with number of hidden units until 64 then we begin to suffer increased error likely 2/2 overfitting.  Interestingly, making hidden layer size smaller than output size actually decreases accuracy, which makes sense as you're boing down all features into 1 neuron and then trying to expand from there.  If you want to learn hierarchical representation of features, likely need to start with larger number of hidden units and then boil down to output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP with two hidden layers\n",
      "\n",
      "1 hidden units\n",
      "Train: 28.09%\tTest: 25.93%\n",
      "\n",
      "8 hidden units\n",
      "Train: 98.49%\tTest: 91.85%\n",
      "\n",
      "16 hidden units\n",
      "Train: 100.00%\tTest: 95.19%\n",
      "\n",
      "32 hidden units\n",
      "Train: 100.00%\tTest: 97.04%\n",
      "\n",
      "64 hidden units\n",
      "Train: 100.00%\tTest: 97.41%\n",
      "\n",
      "128 hidden units\n",
      "Train: 100.00%\tTest: 97.78%\n",
      "\n",
      "256 hidden units\n",
      "Train: 100.00%\tTest: 98.15%\n",
      "\n",
      "512 hidden units\n",
      "Train: 100.00%\tTest: 98.52%\n",
      "\n",
      "1024 hidden units\n",
      "Train: 100.00%\tTest: 98.15%\n"
     ]
    }
   ],
   "source": [
    "neurons = [1,8,16,32,64,128,256,512,1024]\n",
    "accuracies = []\n",
    "aucs = []\n",
    "print(\"MLP with two hidden layers\")\n",
    "for each in neurons:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(each,each), max_iter=1000,activation='relu', random_state=1)\n",
    "    acc = model(nn)\n",
    "    print_acc('{} hidden units'.format(each), acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP with three hidden layers\n",
      "\n",
      "1 hidden units\n",
      "Train: 22.99%\tTest: 19.26%\n",
      "\n",
      "8 hidden units\n",
      "Train: 98.23%\tTest: 90.74%\n",
      "\n",
      "16 hidden units\n",
      "Train: 99.93%\tTest: 93.33%\n",
      "\n",
      "32 hidden units\n",
      "Train: 100.00%\tTest: 95.93%\n",
      "\n",
      "64 hidden units\n",
      "Train: 100.00%\tTest: 97.41%\n",
      "\n",
      "128 hidden units\n",
      "Train: 100.00%\tTest: 96.67%\n",
      "\n",
      "256 hidden units\n",
      "Train: 100.00%\tTest: 98.15%\n",
      "\n",
      "512 hidden units\n",
      "Train: 100.00%\tTest: 98.52%\n",
      "\n",
      "1024 hidden units\n",
      "Train: 100.00%\tTest: 98.52%\n"
     ]
    }
   ],
   "source": [
    "neurons = [1,8,16,32,64,128,256,512,1024]\n",
    "accuracies = []\n",
    "aucs = []\n",
    "print(\"MLP with three hidden layers\")\n",
    "for each in neurons:\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(each,each,each), max_iter=1000,activation='relu', random_state=1)\n",
    "    acc = model(nn)\n",
    "    print_acc('{} hidden units'.format(each), acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of layers improves accuracy with large numbers of hidden units.  In those with smaller number of hidden units, accuracy doesn't improve or begins to overfit.\n",
    "\n",
    "Best models = 2 or 3 layers with 512 units or 3 layers with 1024 units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (unregularized)\n",
      "Train: 100.00%\tTest: 87.78%\n"
     ]
    }
   ],
   "source": [
    "tree_acc = model(DecisionTreeClassifier(random_state=1))\n",
    "print_acc(\"Decision Tree (unregularized)\", tree_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, single decision tree overfit the training data, let's try a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest\n",
      "Train: 100.00%\tTest: 97.41%\n"
     ]
    }
   ],
   "source": [
    "rf_acc =model(RandomForestClassifier(n_estimators=100, random_state=1))\n",
    "print_acc('Random Forest', rf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of random forest classifier with 100 trees a bit better than regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy\n",
    "* 98.5%: NN with 2 layers and 512 hidden units each\n",
    "* 97.4%: Random forest\n",
    "* 97.0%: regularized logistic regression\n",
    "* 94.8%: unregularized logistic regression\n",
    "* 87.8%: unregularized decision tree"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
